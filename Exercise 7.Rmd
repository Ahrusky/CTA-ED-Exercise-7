---
title: "CTA Exercise 7"
author: "Anne Hruska"
date: "2024-03-27"
output: html_document
---

```{r}
install.packages("dataverse")
#load libraries
library(dataverse)
library(dplyr)
```
---------23.1 Intro---------
23.1 Introduction
The hands-on exercise for this week focuses on how to collect and/or sample text information.

In this tutorial, you will learn how to:

Access text information from online corpora
Query text information using different APIs
Scrape text information programmatically
Transcribe text information from audio
Extract text information from images


----------23.2.1-------------------
Let’s take an example dataset in which we might be interested: the UK parliamentary speech data from

We first need to set an en environment variable as so.
```{r}
#set environment variable 
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")
#search out the files that we want by specifying the DOI of the publication data in question. We can find this as a series of numbers and letters that come after “https://doi.org/” as shown below.
dataset <- get_dataset("10.7910/DVN/QDTLYV")
dataset$files[c("filename", "contentType")]
```
Next, choose to get the UK data from these files, which is listed under “UK_data.csv.” We can then download this directly in the following way (this will take some time as the file size is >1GB).
```{r}
#get data from files (TAKES A WHILE)
data <- get_dataframe_by_name(
  "uk_data.csv",
  "10.7910/DVN/QDTLYV",
  .f = function(x) read.delim(x, sep = ","))
```
Of course, we could also download these data manually, by clicking the buttons at the relevant Harvard Dataverse—but it is sometimes useful to build in every step of your data collection to your code documentation, making the analysis entirely programatically reproducible from start to finish.

Note as well that we don’t have to search out specific datasets that we already know about. We can also use the dataverse package to search datasets or dataverses. We can do this very simply in the following way.
```{r}
#use dataverse to search datasets
search_results <- dataverse_search("corpus politics text", type = "dataset", per_page = 10)
## 10 of 37533 results retrieved
#results 1-3?
search_results[,1:3]
```
 Curated corpora
There are, of course, many other sources you might go to for text information. I list some of these that might be of interest below:

Large English-language corpora: https://www.corpusdata.org/
Wikipedia data dumps: https://meta.wikimedia.org/wiki/Data_dumps
English version of dumps here
Scottish Corpus of Texts & Speech: https://www.scottishcorpus.ac.uk/
Corpus of Scottish modern writing: https://www.scottishcorpus.ac.uk/cmsw/
The Manifesto Corpus: https://manifesto-project.wzb.eu/information/documents/corpus
Reddit Pushshift data: https://files.pushshift.io/reddit/
Mediacloud: https://mediacloud.org/
R package: https://github.com/joon-e/mediacloud

------------23.3 Using APIs-------------
In order to use the YouTube API, we’ll first need to get our authorization token. These can be obtained by anybody, with or without an academic profile (i.e., unlike academictwitteR) in previous worksheets.

In order to get you authorization credentials, you can follow this guide. You will need to have an account on the Google Cloud console in order to do this. The main three steps are to:

create a “Project” on the Google Cloud console;
to associate the YouTube API with this Project;
to enable the API keys for the API
Once you have created a Project (here: called “tuberalt1” in my case) you will see a landing screen like this.

```{r}

```





